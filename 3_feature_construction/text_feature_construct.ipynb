{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本特徵建構\n",
    "\n",
    "透過文本中各個文字建構特徵，一個文字都代表一行特徵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 詞袋法\n",
    "\n",
    "將所有文字做集合，成為語料庫(corpus)。並將文件轉成向量的形式，以數值的方式呈現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#載入資料\n",
    "tweets = pd.read_csv('./data/twitter_sentiment.csv', encoding='latin1')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#刪除不需要的ID行\n",
    "del tweets['ItemID']\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConutVectorizer\n",
    "- stop_words:停用詞刪除\n",
    "- min_df:忽略在文件中出現頻率低於臨界值的詞\n",
    "- max_df:保留最多出現在臨界值設定值得詞。例如0.8，表示出現在80%文本的詞將被剃除\n",
    "- ngram_range:幾個字符為一特徵\n",
    "- analyzer:判斷特徵是字詞還是短語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['SentimentText']\n",
    "y = tweets['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "_ = vect.fit_transform(X)\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105545)\n"
     ]
    }
   ],
   "source": [
    "#stopword去除\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "_ = vect.fit_transform(X)\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 31)\n"
     ]
    }
   ],
   "source": [
    "#min_df去除\n",
    "vect = CountVectorizer(min_df=.05)\n",
    "_ = vect.fit_transform(X)\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849)\n"
     ]
    }
   ],
   "source": [
    "#max_df去除\n",
    "vect = CountVectorizer(max_df=.8)\n",
    "_ = vect.fit_transform(X)\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 3219557)\n"
     ]
    }
   ],
   "source": [
    "#ngram建立，最多5個字的短語\n",
    "vect = CountVectorizer(ngram_range=(1, 5))\n",
    "_ = vect.fit_transform(X)\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849)\n"
     ]
    }
   ],
   "source": [
    "#分析器(analyzer)\n",
    "vect = CountVectorizer(analyzer='word')\n",
    "_ = vect.fit_transform(X)\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自訂「詞幹提取」分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interest'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "#觀察還原情形\n",
    "stemmer.stem('interesting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立函數\n",
    "def word_tokenize(text, how='lemma'):\n",
    "    words = text.split(' ')\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'you', 'are', 'veri', 'interest']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#觀察效果\n",
    "word_tokenize(\"hello you are very interesting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 154397)\n"
     ]
    }
   ],
   "source": [
    "#將分詞器傳入分析器參數\n",
    "vect = CountVectorizer(analyzer=word_tokenize)\n",
    "_ = vect.fit_transform(X)\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 向量化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849) 6.613194267305311e-05\n"
     ]
    }
   ],
   "source": [
    "#原始的CountVectorizer，輸出shape與平均值\n",
    "vect = CountVectorizer()\n",
    "_ = vect.fit_transform(X)\n",
    "print(_.shape, _[0,:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849) 2.1863060975751192e-05\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF建立向量。兩者shape相同，但數值不同\n",
    "vect = TfidfVectorizer()\n",
    "_ = vect.fit_transform(X)\n",
    "print(_.shape, _[0,:].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機器學習管線應用文本特徵\n",
    "\n",
    "特徵多時，要使用有效率的分類方法，例如「單純貝式(Naive Bayes)」\n",
    "\n",
    "步驟:\n",
    "1. 先將文本轉換成特徵表達形式\n",
    "2. 利用單純貝式進行正面或負面情緒分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.564632\n",
       "0    0.435368\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#計算空準確率\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#設置管線參數\n",
    "pipe_params = {'vect__ngram_range':[(1,1), (1,2)], 'vect__max_features':[1000, 10000], 'vect__stop_words':[None, 'english']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#產生實體管線，先建立特徵，再分類\n",
    "pipe = Pipeline([('vect', CountVectorizer()), ('classify', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7558931564507154 {'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "#網格搜尋\n",
    "grid = GridSearchCV(pipe, pipe_params)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_score_, grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureUnion\n",
    "\n",
    "此模組可以使用多個不同文本特徵來表示文本，像是前面所提及的CountVectorizer與TfidfVectorizer兩者結合。特徵數量會變多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = FeatureUnion([('tfidf_vect', TfidfVectorizer()), ('count_vect', CountVectorizer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 211698)\n"
     ]
    }
   ],
   "source": [
    "_ = featurizer.fit_transform(X)\n",
    "print(_.shape)\n",
    "#可以看到特徵數量變為兩倍，那是因為結合兩種特徵表達方式，因此變為兩倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 400)\n"
     ]
    }
   ],
   "source": [
    "#改變參數數量，我們可以透過設定參數使得數量不要那麼多，以下為範例，最後輸出參數數量為400個\n",
    "featurizer.set_params(tfidf_vect__max_features=100, count_vect__ngram_range=(1,2), count_vect__max_features=300)\n",
    "\n",
    "_= featurizer.fit_transform(X)\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#設定更為完整的參數\n",
    "pipe_params = {'featurizer__count_vect__ngram_range':[(1, 1), (1, 2)],\n",
    "               'featurizer__count_vect__max_features':[1000, 10000],\n",
    "               'featurizer__count_vect__stop_words':[None, 'english'],\n",
    "               'featurizer__tfidf_vect__ngram_range':[(1, 1), (1, 2)],\n",
    "               'featurizer__tfidf_vect__max_features':[1000, 10000],\n",
    "               'featurizer__tfidf_vect__stop_words':[None, 'english']}\n",
    "\n",
    "pipe = Pipeline([('featurizer', featurizer), ('classify', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7580933924767184 {'featurizer__count_vect__max_features': 10000, 'featurizer__count_vect__ngram_range': (1, 2), 'featurizer__count_vect__stop_words': None, 'featurizer__tfidf_vect__max_features': 10000, 'featurizer__tfidf_vect__ngram_range': (1, 1), 'featurizer__tfidf_vect__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(pipe, pipe_params)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_score_, grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
